# NeuralDoc AI â€” Production-Ready RAG SaaS Platform

ğŸš€ Live Demo: [Your Deployment Link Here]  
ğŸ³ Dockerized | ğŸ” JWT Secured | â˜ Pinecone Integrated | ğŸ¤– Multi-LLM Support  

---

## ğŸ“Œ Overview

NeuralDoc AI is a production-ready Retrieval-Augmented Generation (RAG) platform that enables users to upload documents and query them using Large Language Models (LLMs).

Unlike basic RAG demos, this system is designed with real-world engineering principles including:

- Service-layer architecture
- LLM provider abstraction
- Cloud vector database integration
- Authentication
- Dockerized deployment
- Observability tracking

This project simulates a real SaaS AI backend system.

---

## ğŸ§  Problem Statement

Large Language Models:

- Hallucinate without context
- Cannot access private documents
- Lack retrieval memory
- Are not production-secure by default

Most student RAG projects:
- Use global variables
- Lack authentication
- Do not scale
- Are not deployable
- Have no system design thinking

NeuralDoc AI solves these problems by providing a modular, secure, scalable RAG backend system.

---

## ğŸ— System Architecture

![Architecture Diagram](assets/architecturaldiagram.png)

Key Design Decisions:

- Service layer removes global state
- LLM abstraction allows provider switching via environment variables
- Cloud vector database supports scaling
- JWT authentication protects endpoints
- Docker ensures deployment consistency

---

## ğŸ”§ Tech Stack

**Backend**
- FastAPI
- Python
- SQLAlchemy
- JWT Authentication

**AI & ML**
- Sentence Transformers (all-MiniLM-L6-v2)
- Pinecone (Cloud Vector DB)
- FAISS (Local Dev)
- Ollama (Local LLM)
- Groq / Gemini (Cloud LLM)

**DevOps**
- Docker
- Environment variable validation
- MLflow (observability)

---

## ğŸš€ Core Features

âœ… Document upload & indexing  
âœ… Semantic similarity retrieval  
âœ… Multi-LLM provider support  
âœ… Cloud vector DB integration  
âœ… JWT-protected endpoints  
âœ… Dockerized deployment  
âœ… Environment-based configuration  
âœ… Clean Git versioning workflow  



---

## ğŸ” Security

- Password hashing (bcrypt)
- JWT-based authentication
- Protected endpoints (/ask, /upload)
- Config validation on startup
- No hardcoded secrets

---

## ğŸ“ˆ Version Evolution

| Version | Feature |
|----------|----------|
| v1.0 | Core RAG implementation |
| v1.1 | Observability with MLflow |
| v2.0 | Persistent FAISS storage |
| v2.1 | Service layer refactor |
| v3 | LLM abstraction layer |
| v4 | Pinecone cloud integration |
| v5 | JWT authentication |

This project evolved incrementally using feature branches and pull request workflow.

---

## âš–ï¸ Local vs Production Strategy

| Component | Local | Production |
|------------|--------|-------------|
| LLM | Ollama | Groq / Gemini |
| Vector DB | FAISS | Pinecone |
| Deployment | Localhost | Docker Cloud Deploy |

This separation ensures development flexibility and production scalability.

---

## ğŸ“Š Scaling Considerations

- Stateless backend design
- External vector DB scaling
- Provider abstraction allows LLM switching
- Docker-ready for horizontal scaling
- Authentication supports multi-user extension

---

## ğŸ–¥ Demo Flow

1. Register
2. Login
3. Upload document
4. Ask contextual questions
5. Receive AI-generated answer grounded in retrieved context


### ğŸ“„ Upload Document
Drop a PDF into the Knowledge Base panel and click **Upload & Index**.

![Upload Document](assets/Load.png)
---

###  ğŸ’¬ Ask Contextual Questions
Type your question in the chat input and hit send.

![Ask Question](assets/ask.png)


## ğŸ¯ What I Learned

- Production-ready RAG architecture
- AI system design tradeoffs
- LLM abstraction patterns
- Cloud vector database integration
- Docker deployment debugging
- Git branch discipline and versioning strategy
- Handling quota limits and provider fallback

---

## ğŸ“Œ Future Improvements

- Rate limiting per user
- Usage tracking dashboard
- Streaming responses
- Background embedding worker
- Admin analytics panel

---

## ğŸ‘¨â€ğŸ’» Author

Uday Bansal  
B.Tech Computer Science (AIML)  
Aspiring Full Stack AI Engineer